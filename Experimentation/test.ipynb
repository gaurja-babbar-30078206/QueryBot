{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Inbuilt imports\n",
    "import os ## use for doc extension, if already using\n",
    "import pathlib # creates single var\n",
    "\n",
    "\n",
    "# text and pdf covered, also covers Images(jpg, png), have to see how??\n",
    "from langchain_community.document_loaders import UnstructuredFileLoader \n",
    "# pdf\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "from contanst import available_docs\n",
    "from utils import blog\n",
    "from time import time\n",
    "\n",
    "def print_doc_list():\n",
    "    print(\"List of available documents:\")\n",
    "    for count in range(len(available_docs)):\n",
    "        print(f\"[{count}] {available_docs[count].file_name}\")\n",
    "   \n",
    "class DocumentReader:\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Initialising text splitter\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "        print_doc_list()\n",
    "        chosen_doc = int(input(\"Enter index of chosen document: \"))\n",
    "        self.path = available_docs[chosen_doc].file_path\n",
    "        blog(f\"File chosen -----> {self.path}\")\n",
    "                  \n",
    "              \n",
    "    # get the file extension\n",
    "    def get_file_extension(self):\n",
    "        return pathlib.Path(self.path).suffix\n",
    "    \n",
    "    # returns loaded document\n",
    "    def get_document(self):\n",
    "        file_ext = self.get_file_extension()\n",
    "        match file_ext:\n",
    "            case '.pdf':\n",
    "                loader = UnstructuredFileLoader(self.path)               \n",
    "            case '.txt':\n",
    "                loader = UnstructuredFileLoader(self.path)\n",
    "            case _:\n",
    "                print('Format of the document is not supported')    \n",
    "        return loader.load()\n",
    "     \n",
    "    # Splitting documents\n",
    "    def split_documents(self):\n",
    "        docs = self.get_document()\n",
    "        return self.text_splitter.split_documents(docs)\n",
    "        \n",
    "            \n",
    "    # creates vector embeddings and stores in vector store    \n",
    "    def load_document(self,embeddings):\n",
    "        docs = self.split_documents()\n",
    "        start_time = time()\n",
    "        vector_store =  Chroma.from_documents(documents=docs, embedding = embeddings)\n",
    "        blog(f\"Vector Store Creation time ----->{time() - start_time}\")       \n",
    "        return vector_store     \n",
    "        \n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\30078206\\AppData\\Local\\anaconda3\\envs\\myenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "## need an LLM\n",
    "from langchain_community.llms.ctransformers import CTransformers\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "dir = r\"D:\\OneDrive - Adani\\Desktop\\LEARNING_FOLDER\\_Kolkata_2024\\1_LLM\\local_downloaded_models\"\n",
    "file_name = \"llama-2-7b-chat.Q6_K.gguf\"\n",
    "llm =  CTransformers( model= dir, model_file = file_name, callbacks=[StreamingStdOutCallbackHandler()], config = {\"context_length\": 16000, \"max_new_tokens\": 3000})\n",
    "embed_llm = HuggingFaceEmbeddings(\n",
    "            model_name = r\"D:\\OneDrive - Adani\\Desktop\\LEARNING_FOLDER\\_Kolkata_2024\\1_LLM\\local_downloaded_models\\embedding_models\\gte-base-en-v1.5\",\n",
    "            show_progress = True,\n",
    "            model_kwargs = {\"trust_remote_code\": True})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nMapReduceDocumentsChain\\nMapRerankDocumentsChain\\nReduceDocumentsChain\\nRefineDocumentsChain\\nStuffDocumentsChain\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Question Answering Chaing\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    messages= [\n",
    "        (\"system\", \"\"\"\n",
    "         Answer the user's question from the following context: {context}\n",
    "         Question: {input} \n",
    "         \"\"\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "## creating stuff doc chain\n",
    "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
    "from langchain.chains.combine_documents.map_reduce import MapReduceDocumentsChain\n",
    "from langchain.chains.combine_documents.reduce import ReduceDocumentsChain\n",
    "from langchain.chains.combine_documents.map_rerank import MapRerankDocumentsChain\n",
    "from langchain.chains.combine_documents.refine import RefineDocumentsChain\n",
    "from langchain.chains import create_history_aware_retriever\n",
    "\n",
    "llm_chain = prompt | llm\n",
    "\n",
    "chain = StuffDocumentsChain(\n",
    "    llm_chain= llm_chain,\n",
    "    document_variable_name= \"context\"\n",
    ")\n",
    "\n",
    "chain.invoke({\"input\": \"\"})\n",
    "create_history_aware_retriever(llm , )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['chat_history', 'input'], input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='\\n         Given a chat history and the latest user question, do this step by step, first check if\\n         the latest user question references anything in the chat history context,if so then reformulate the latest \\n         user question into a question that could be understood without the chat history. If the latest question\\n         does not reference anything in the chat history, return that question as it is, without \\n         any change.\\n         ')), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}'))])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "         Given a chat history and the latest user question, do this step by step, first check if\n",
    "         the latest user question references anything in the chat history context,if so then reformulate the latest \n",
    "         user question into a question that could be understood without the chat history. If the latest question\n",
    "         does not reference anything in the chat history, return that question as it is, without \n",
    "         any change.\n",
    "         \"\"\" \n",
    "         \n",
    "contextual_prompt = ChatPromptTemplate.from_messages(\n",
    "    messages= [\n",
    "        (\"system\", prompt ),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\")\n",
    "\n",
    "    ]\n",
    "    \n",
    "    ## New query to be generated if it is related to chat history\n",
    "    \n",
    ")         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ChatPromptTemplate(input_variables=['chat_history', 'input'], input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='\\n         Given a chat history and the latest user question, do this step by step, first check if\\n         the latest user question references anything in the chat history context,if so then reformulate the latest \\n         user question into a question that could be understood without the chat history. If the latest question\\n         does not reference anything in the chat history, return that question as it is, without \\n         any change.\\n         ')), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}'))])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
