{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective: To use gguf model locally with langchain \n",
    "\n",
    "    embedding: HuggingFaceEmbedding(sentence-transformers/all-mpnet-base-v2)\n",
    "    \n",
    "    vector store: Chroma\n",
    "    \n",
    "    retriever: from vector store\n",
    "    \n",
    "    llm:\n",
    "        1)mistral_7B_v0.3\n",
    "        2)capybarahermes-2.5-mistral-7b.Q3_K_L.gguf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Loader\n",
    "    pdf loader : langchain inbuilt document loader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "file_path = (r\"D:\\OneDrive - Adani\\Desktop\\LEARNING_FOLDER\\_Kolkata_2024\\1_LLM\\3_Text_query_bot\\_docs\\Leave_Policy_2024.pdf\")\n",
    "loader = PyPDFLoader(file_path)\n",
    "pages = loader.load_and_split()\n",
    "len(pages)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split\n",
    "    smaller chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(pages)\n",
    "len(splits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_chroma import Chroma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create vector store\n",
    "    stores embeddings of Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_chroma.vectorstores.Chroma at 0x258285366d0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_embeddings = HuggingFaceEmbeddings(model_name= r\"D:\\OneDrive - Adani\\Desktop\\LEARNING_FOLDER\\_Kolkata_2024\\1_LLM\\local_downloaded_models\\embedding_models\\gte-base\")\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=new_embeddings)\n",
    "vectorstore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_type=\"similarity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM Used:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1)Model: mistral_7B_v0.3\n",
    "    Langchain + CTransformer\n",
    "    Size: 4.07 GB \n",
    "    RAM usage: how to find??\n",
    "    Response time:\n",
    "    \n",
    "- quantized model with langchain\n",
    "- on cpu without nvidia gpu\n",
    "- cpu -- 16 gb RAM\n",
    "- download the gguf model\n",
    "- What is quantization --> the higger bit quant --> more ram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import CTransformers\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "model_dir =  r\"D:\\OneDrive - Adani\\Desktop\\LEARNING_FOLDER\\_Kolkata_2024\\1_LLM\\local_downloaded_models\\mistral_7B_v0.3\"\n",
    "model_file = \"Mistral-7B-Instruct-v0.3.Q4_K_M.gguf\"\n",
    "\n",
    "config = {'context_length': 16000, 'max_new_tokens': 1600}\n",
    "\n",
    "\n",
    "llm = CTransformers(model= model_dir, model_file = model_file, callbacks=[StreamingStdOutCallbackHandler()], config= config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2)Model:capybarahermes-2.5-mistral-7b.Q3_K_L.gguf\n",
    "\n",
    "    Langchain + CTransformer\n",
    "    Size: 3.82 GB \n",
    "    RAM usage: 6.02 GB\n",
    "    Response time:\n",
    "\n",
    "\n",
    "Model Specs:\n",
    "- context_length: 32768\n",
    "\n",
    "def=> number of tokens or words that model takes into account when generating a response\n",
    "\n",
    "- max_new_tokens: 32768 \n",
    "\n",
    "def => max number of tokens that can be generated. helps to control the length of generated output\n",
    "    \n",
    "(+)Note:\n",
    "- if less max tokens, the processing will be faster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import CTransformers\n",
    "from langchain import PromptTemplate\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "model_dir =  r\"D:\\OneDrive - Adani\\Desktop\\LEARNING_FOLDER\\_Kolkata_2024\\1_LLM\\local_downloaded_models\"\n",
    "model_file = \"capybarahermes-2.5-mistral-7b.Q3_K_M.gguf\"\n",
    "\n",
    "config = {'context_length': 16000, 'max_new_tokens': 1600}\n",
    "\n",
    "\n",
    "llm = CTransformers(model= model_dir, model_file = model_file, callbacks=[StreamingStdOutCallbackHandler()], config= config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) llama-2-7b-chat.Q6_K.gguf\n",
    "download link: https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF/tree/main\n",
    "Model specifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import CTransformers\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "model_dir =  r\"D:\\OneDrive - Adani\\Desktop\\LEARNING_FOLDER\\_Kolkata_2024\\1_LLM\\local_downloaded_models\"\n",
    "model_file = \"llama-2-7b-chat.Q6_K.gguf\"\n",
    "\n",
    "config = {'context_length': 16000, 'max_new_tokens': 1600}\n",
    "\n",
    "\n",
    "llm = CTransformers(model= model_dir, model_file = model_file, callbacks=[StreamingStdOutCallbackHandler()], config= config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.invoke(\"You are a better AI assistant you know\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Terms:\n",
    "- Multiqueryretriever: \n",
    "    - automates process of tuning\n",
    "    - to generate multiple queries from different perspective \n",
    "    - for each query- returns relevant documents,, takes union across all \n",
    "    - Overcomes the limitation of distance based retrieval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Retrievers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG prompt\n",
    "template = \"\"\"Answer the question based ONLY on the following context:\n",
    "{context}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig()\n",
    "logging.getLogger(\"langchain.retrievers.multi_query\").setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever()\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Of course! Based on the provided context, here are five key points summarizing the Leave Policy for Adani Group employees:\n",
      "1. Employees can avail up to 5.4 days of Special Leave (SL) without any limit.\n",
      "2. SL can be combined with Paternity Leave/Maternity Leave in case of prolonged illness, subject to submission of medical certificates.\n",
      "3. Employees have to compulsorily avail at least 15 days of Privilege Leave (PL) in a block of two years for rejuvenation.\n",
      "4. PL is credited to the employee's Leave account at the end of each Leave Year, and unavailed PL will lapse in the same year.\n",
      "5. The Sanctioning Authority for leave approval is usually the reporting manager of the employee or any person authorized by the organization."
     ]
    }
   ],
   "source": [
    "chain = (\n",
    "    {\"context\": retriever| format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "result=chain.invoke(\"Give me summary in 5 bullet points only\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
