{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective: To use gguf model locally with langchain \n",
    "\n",
    "    embedding: HuggingFaceEmbedding(sentence-transformers/all-mpnet-base-v2)\n",
    "    \n",
    "    vector store: Chroma\n",
    "    \n",
    "    retriever: from vector store\n",
    "    \n",
    "    llm:\n",
    "        1)mistral_7B_v0.3\n",
    "        2)capybarahermes-2.5-mistral-7b.Q3_K_L.gguf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Loader\n",
    "    pdf loader : langchain inbuilt document loader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "file_path = (r\"D:\\OneDrive - Adani\\Desktop\\LEARNING_FOLDER\\_Kolkata_2024\\1_LLM\\3_Text_query_bot\\_docs\\Leave_Policy_2024.pdf\")\n",
    "loader = PyPDFLoader(file_path)\n",
    "pages = loader.load_and_split()\n",
    "len(pages)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split\n",
    "    smaller chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(pages)\n",
    "len(splits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_chroma import Chroma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create vector store\n",
    "    stores embeddings of Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_embeddings = HuggingFaceEmbeddings(model_name= r\"D:\\OneDrive - Adani\\Desktop\\LEARNING_FOLDER\\_Kolkata_2024\\1_LLM\\local_downloaded_models\\embedding_models\\gte-base\")\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=new_embeddings)\n",
    "vectorstore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_type=\"similarity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM Used:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1)Model: mistral_7B_v0.3\n",
    "    Langchain + CTransformer\n",
    "    Size: 4.07 GB \n",
    "    RAM usage: how to find??\n",
    "    Response time:\n",
    "    \n",
    "- quantized model with langchain\n",
    "- on cpu without nvidia gpu\n",
    "- cpu -- 16 gb RAM\n",
    "- download the gguf model\n",
    "- What is quantization --> the higger bit quant --> more ram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import CTransformers\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "model_dir =  r\"D:\\OneDrive - Adani\\Desktop\\LEARNING_FOLDER\\_Kolkata_2024\\1_LLM\\local_downloaded_models\\mistral_7B_v0.3\"\n",
    "model_file = \"Mistral-7B-Instruct-v0.3.Q4_K_M.gguf\"\n",
    "\n",
    "config = {'context_length': 16000, 'max_new_tokens': 1600}\n",
    "\n",
    "\n",
    "llm = CTransformers(model= model_dir, model_file = model_file, callbacks=[StreamingStdOutCallbackHandler()], config= config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2)Model:capybarahermes-2.5-mistral-7b.Q3_K_L.gguf\n",
    "\n",
    "    Langchain + CTransformer\n",
    "    Size: 3.82 GB \n",
    "    RAM usage: 6.02 GB\n",
    "    Response time:\n",
    "\n",
    "\n",
    "Model Specs:\n",
    "- context_length: 32768\n",
    "\n",
    "def=> number of tokens or words that model takes into account when generating a response\n",
    "\n",
    "- max_new_tokens: 32768 \n",
    "\n",
    "def => max number of tokens that can be generated. helps to control the length of generated output\n",
    "    \n",
    "(+)Note:\n",
    "- if less max tokens, the processing will be faster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import CTransformers\n",
    "from langchain import PromptTemplate\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "model_dir =  r\"D:\\OneDrive - Adani\\Desktop\\LEARNING_FOLDER\\_Kolkata_2024\\1_LLM\\local_downloaded_models\"\n",
    "model_file = \"capybarahermes-2.5-mistral-7b.Q3_K_M.gguf\"\n",
    "\n",
    "config = {'context_length': 16000, 'max_new_tokens': 1600}\n",
    "\n",
    "\n",
    "llm = CTransformers(model= model_dir, model_file = model_file, callbacks=[StreamingStdOutCallbackHandler()], config= config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) llama-2-7b-chat.Q6_K.gguf\n",
    "download link: https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF/tree/main\n",
    "Model specifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import CTransformers\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "model_dir =  r\"D:\\OneDrive - Adani\\Desktop\\LEARNING_FOLDER\\_Kolkata_2024\\1_LLM\\local_downloaded_models\"\n",
    "model_file = \"llama-2-7b-chat.Q6_K.gguf\"\n",
    "\n",
    "config = {'context_length': 16000, 'max_new_tokens': 1600}\n",
    "\n",
    "\n",
    "llm = CTransformers(model= model_dir, model_file = model_file, callbacks=[StreamingStdOutCallbackHandler()], config= config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.invoke(\"You are a better AI assistant you know\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Terms:\n",
    "- Multiqueryretriever: \n",
    "    - automates process of tuning\n",
    "    - to generate multiple queries from different perspective \n",
    "    - for each query- returns relevant documents,, takes union across all \n",
    "    - Overcomes the limitation of distance based retrieval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Retrievers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG prompt\n",
    "template = \"\"\"Answer the question based ONLY on the following context:\n",
    "{context}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig()\n",
    "logging.getLogger(\"langchain.retrievers.multi_query\").setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever()\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory:\n",
    "1) ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory(\n",
    "    memory_key= \"chat_history\",\n",
    "    return_messages= True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For Memory LCEL may not work\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.chains.conversation.base import ConversationChain\n",
    "from langchain.chains import RetrievalQA,  ConversationalRetrievalChain\n",
    "from langchain.chains.retrieval_qa.base import RetrievalQA\n",
    "from langchain.chains.conversational_retrieval.base import ConversationalRetrievalChain\n",
    "\n",
    "conversation = ConversationalRetrievalChain.from_llm(\n",
    "    llm= llm,\n",
    "    retriever=retriever,\n",
    "    return_source_documents =  True,\n",
    "    return_generated_question = True,\n",
    ")\n",
    "\n",
    "# chain = {\"context\": retriever| format_docs, \"question\": RunnablePassthrough} | conversation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "contextualize_q_system_prompt = \"\"\"Given a chat history and the latest user question \\\n",
    "which might reference context in the chat history, formulate a standalone question \\\n",
    "which can be understood without the chat history. Do NOT answer the question, \\\n",
    "just reformulate it if needed and otherwise return it as is.\"\"\"\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    llm, retriever, contextualize_q_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "qa_system_prompt = \"\"\"You are an assistant for question-answering tasks. \\\n",
    "Use the following pieces of retrieved context to answer the question. \\\n",
    "If you don't know the answer, just say that you don't know. \\\n",
    "Use three sentences maximum and keep the answer concise.\\\n",
    "\n",
    "{context}\"\"\"\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", qa_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "\n",
    "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = []\n",
    "conversation.invoke({\"question\": \"Give me summary in 5 bullet points\", \"chat_history\":chat_history })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = conversation.invoke({\"question\": \"Tell me more about PL\", \"chat_history\":chat_history })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer=output['answer']\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = output['question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer=output['answer']\n",
    "question = output['question']\n",
    "from langchain_core.messages import HumanMessage\n",
    "chat_history.extend([HumanMessage(content = question),answer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\" Privilege Leave (PL) is a type of leave that employees earn based on their service. They are entitled to 21 days of PL in a Leave Year 2. Their earning rate for PL is 1.75 days for every month of service rendered. PL can be availed in units of 0.5 day. Employees must compulsorily avail at least 15 days of PL in a block of 2 years to rejuvenate themselves. In the case of employees who have recently joined, the first block of 2 years starts from January when they earn 21 days of leave. Unavailed compulsory PL will lapse in the Leave Year following the year in which compulsory PL was due. Advance PL can be requested through appropriate channels and will be settled against the PL balance that will be credited in the next Leave Year. However, if leave is credited and availed in advance of entitlement or earning, it will be recovered from employees upon leaving the company's service unless they have earned that leave.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(chat_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = conversation.invoke({\"question\": \"Tell me more about the last question \", \"chat_history\":chat_history })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_chain = retriever\n",
    "docs = retriever_chain.invoke(\"Give me summary in 5 bullet points\")\n",
    "docs\n",
    "total_pages = 0\n",
    "for pages in docs:\n",
    "    total_pages = total_pages + len(pages.page_content)\n",
    "total_pages    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_chain = retriever | format_docs\n",
    "docs = retriever_chain.invoke(\"Give me summary in 5 bullet points\")\n",
    "len(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_chain = retriever_chain | conversation\n",
    "final_chain.invoke(input= \"dfdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "    {\"context\": retriever| format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "result=chain.invoke(\"Give me summary in 5 bullet points only\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
