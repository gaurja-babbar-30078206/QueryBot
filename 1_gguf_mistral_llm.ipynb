{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective: To use gguf model locally with langchain \n",
    "\n",
    "    embedding: HuggingFaceEmbedding(sentence-transformers/all-mpnet-base-v2)\n",
    "    \n",
    "    vector store: Chroma\n",
    "    \n",
    "    retriever: from vector store\n",
    "    \n",
    "    llm:\n",
    "        1)mistral_7B_v0.3\n",
    "        2)capybarahermes-2.5-mistral-7b.Q3_K_L.gguf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Loader\n",
    "    pdf loader : langchain inbuilt document loader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "file_path = (r\"D:\\OneDrive - Adani\\Desktop\\LEARNING_FOLDER\\_Kolkata_2024\\1_LLM\\3_Text_query_bot\\_docs\\Leave_Policy_2024.pdf\")\n",
    "loader = PyPDFLoader(file_path)\n",
    "pages = loader.load_and_split()\n",
    "len(pages)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split\n",
    "    smaller chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(pages)\n",
    "len(splits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_chroma import Chroma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create vector store\n",
    "    stores embeddings of Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\30078206\\AppData\\Local\\anaconda3\\envs\\myenv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<langchain_chroma.vectorstores.Chroma at 0x1e72ab625d0>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_embeddings = HuggingFaceEmbeddings(model_name= \"sentence-transformers/all-mpnet-base-v2\")\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=new_embeddings)\n",
    "vectorstore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_type=\"similarity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM Used:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1)Model: mistral_7B_v0.3\n",
    "    Langchain + CTransformer\n",
    "    Size: 4.07 GB \n",
    "    RAM usage: how to find??\n",
    "    Response time:\n",
    "    \n",
    "- quantized model with langchain\n",
    "- on cpu without nvidia gpu\n",
    "- cpu -- 16 gb RAM\n",
    "- download the gguf model\n",
    "- What is quantization --> the higger bit quant --> more ram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import CTransformers\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "model_dir =  r\"D:\\OneDrive - Adani\\Desktop\\LEARNING_FOLDER\\_Kolkata_2024\\1_LLM\\local_downloaded_models\\mistral_7B_v0.3\"\n",
    "model_file = \"Mistral-7B-Instruct-v0.3.Q4_K_M.gguf\"\n",
    "\n",
    "config = {'context_length': 16000, 'max_new_tokens': 16000}\n",
    "\n",
    "\n",
    "llm = CTransformers(model= model_dir, model_file = model_file, callbacks=[StreamingStdOutCallbackHandler()], config= config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2)Model:capybarahermes-2.5-mistral-7b.Q3_K_L.gguf\n",
    "\n",
    "    Langchain + CTransformer\n",
    "    Size: 3.82 GB \n",
    "    RAM usage: 6.02 GB\n",
    "    Response time:\n",
    "\n",
    "\n",
    "Model Specs:\n",
    "- context_length: 32768\n",
    "\n",
    "def=> number of tokens or words that model takes into account when generating a response\n",
    "\n",
    "- max_new_tokens: 32768 \n",
    "\n",
    "def => max number of tokens that can be generated. helps to control the length of generated output\n",
    "    \n",
    "(+)Note:\n",
    "- if less max tokens, the processing will be faster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import CTransformers\n",
    "from langchain import PromptTemplate\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "model_dir =  r\"D:\\OneDrive - Adani\\Desktop\\LEARNING_FOLDER\\_Kolkata_2024\\1_LLM\\local_downloaded_models\"\n",
    "model_file = \"capybarahermes-2.5-mistral-7b.Q3_K_M.gguf\"\n",
    "\n",
    "config = {'context_length': 16000, 'max_new_tokens': 16000}\n",
    "\n",
    "\n",
    "llm = CTransformers(model= model_dir, model_file = model_file, callbacks=[StreamingStdOutCallbackHandler()], config= config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "QUERY_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"You are an AI language model assistant. Your task is to generate five\n",
    "    different versions of the given user question to retrieve relevant documents from\n",
    "    a vector database. By generating multiple perspectives on the user question, your\n",
    "    goal is to help the user overcome some of the limitations of the distance-based\n",
    "    similarity search. Provide these alternative questions separated by newlines.\n",
    "    Original question: {question}\"\"\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Terms:\n",
    "- Multiqueryretriever: \n",
    "    - automates process of tuning\n",
    "    - to generate multiple queries from different perspective \n",
    "    - for each query- returns relevant documents,, takes union across all \n",
    "    - Overcomes the limitation of distance based retrieval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Retrievers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG prompt\n",
    "template = \"\"\"Answer the question based ONLY on the following context:\n",
    "{context}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig()\n",
    "logging.getLogger(\"langchain.retrievers.multi_query\").setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever()\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Privilege Leave (PL) is an earned leave that can be availed by employees in units of half a day through appropriate channels. The balance of PL earned will be credited to the employee's account annually. Any PL that is taken as advance will be settled against the PL balance credited in the next leave year. If an employee leaves the company, any PL availed as an advance prior to leaving will need to be recovered from them unless they have already earned that leave. For employees who have recently joined, the first block of 2 years starts from January, during which they earn 21 days of PL. Any unavailed compulsory PL will lapse in the Leave Year following the year in which it was due. The maximum balance of leave at the beginning of any leave year cannot exceed 90 days; any excess amount will be automatically encashed as per procedures and rules on leave encashment. An employee may opt to encash their leave credit, subject to maintaining a balance of 30 days PL at credit. Employees can request PL in advance but should note that the balance of PL will not exceed the balance of earned leave from the next leave year."
     ]
    }
   ],
   "source": [
    "chain = (\n",
    "    {\"context\": retriever| format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "result=chain.invoke(\"Tell me more about PL\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Answering option ONLY based on the text given: Privilege Leave is an earned leave that can be availed in advance and settled against the PL balance in the next leave year.'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
