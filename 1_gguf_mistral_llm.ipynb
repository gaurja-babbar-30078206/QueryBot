{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective: To use gguf model locally with langchain \n",
    "OllamaLLm and HuggingFaceEmbedding: mistralai/Mistral-7B-Instruct-v04"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Loader\n",
    "    pdf loader : langchain inbuilt document loader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "file_path = (r\"D:\\OneDrive - Adani\\Desktop\\LEARNING_FOLDER\\_Kolkata_2024\\1_LLM\\3_Text_query_bot\\_docs\\Leave_Policy_2024.pdf\")\n",
    "loader = PyPDFLoader(file_path)\n",
    "pages = loader.load_and_split()\n",
    "len(pages)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split\n",
    "    smaller chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(pages)\n",
    "len(splits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_chroma import Chroma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create vector store\n",
    "    stores embeddings of Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cutting down time whilst createing embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\30078206\\AppData\\Local\\anaconda3\\envs\\myenv\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "c:\\Users\\30078206\\AppData\\Local\\anaconda3\\envs\\myenv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<langchain_chroma.vectorstores.Chroma at 0x22241f09750>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Not local\n",
    "new_embeddings = HuggingFaceEmbeddings(model_name= \"sentence-transformers/all-mpnet-base-v2\")\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=new_embeddings)\n",
    "vectorstore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_type=\"similarity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\30078206\\AppData\\Local\\anaconda3\\envs\\myenv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leave Policy  \n",
      " Version  HRLP24  \n",
      "Issue Date  1-Apr-2024 \n",
      "Effective Date  1-Jan-2024  \n",
      "This version supersedes all previous versions with a lesser version number  \n",
      " \n",
      "Page 1 of 4 \n",
      " \n",
      "1.0 OBJECTIVE : Leave is a period of rest and relaxation  provided at intervals during  \n",
      "work periods. For convenience in terms of reference, it is divided into categories  \n",
      "such as Privilege Leave (PL), Casual Leave (CL), Sick Leave (SL) and Special Leave  \n",
      "    \n",
      "2.0 DEFINITIONS : \n",
      "2.1 Leave Year:  The Leave Year is the calendar year from 1 Jan uary  to 31 Dec ember . \n",
      "All calculations of earning, credit, availment, balance etc. of leave will be done \n",
      "with respect to each Leave Year  \n",
      "2.2 Entitlement: This refers to the amount , or the numb er of days  of Leave (of \n",
      "different types ) that an employee gets in a Leave Year as part of conditions of \n",
      "service , subject to certain conditions such as full attendance on full salary. For \n",
      "example,  the entitlement of PL is 21 days in a Leave Year\n"
     ]
    }
   ],
   "source": [
    "# relevant_docs = retriever.get_relevant_documents(\"Definition\")\n",
    "# print(relevant_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM Used:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1)Model: mistral_7B_v0.3\n",
    "    Langchain + CTransformer\n",
    "    Size: 4.07 GB \n",
    "    RAM usage: how to find??\n",
    "    Response time:\n",
    "    \n",
    "- quantized model with langchain\n",
    "- on cpu without nvidia gpu\n",
    "- cpu -- 16 gb RAM\n",
    "- download the gguf model\n",
    "- What is quantization --> the higger bit quant --> more ram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import CTransformers\n",
    "from langchain import PromptTemplate\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "model_dir =  r\"D:\\OneDrive - Adani\\Desktop\\LEARNING_FOLDER\\_Kolkata_2024\\1_LLM\\local_downloaded_models\\mistral_7B_v0.3\"\n",
    "model_file = \"Mistral-7B-Instruct-v0.3.Q4_K_M.gguf\"\n",
    "\n",
    "config = {'context_length': 2048}\n",
    "\n",
    "\n",
    "llm = CTransformers(model= model_dir, model_file = model_file, callbacks=[StreamingStdOutCallbackHandler()], config= config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "In the tranquil skies so bright,\n",
      "A symphony of feathers take flight.\n",
      "Their melodious songs fill the air,\n",
      "A gentle whisper from afar.\n",
      "\n",
      "From the smallest hummingbird's flit,\n",
      "To the majestic eagle's might,\n",
      "Each bird in its own way, so divine,\n",
      "An ode to life and the great design.\n",
      "\n",
      "The robin, a harbinger of spring,\n",
      "Its red breast adorning its wings,\n",
      "Sings with joy as it takes its place,\n",
      "In nature's grand and wondrous space.\n",
      "\n",
      "The sparrow, humble yet steadfast,\n",
      "Lives in harmony, never boastful or hasty,\n",
      "A lesson for us all to learn,\n",
      "To cherish what is precious, but still unseen.\n",
      "\n",
      "The swift and graceful swallows fly,\n",
      "With a speed that leaves the eye to wonder why,\n",
      "Like shadows darting across the sunlit field,\n",
      "Their agility and beauty never fail to yield.\n",
      "\n",
      "The soaring eagle on its wings so wide,\n",
      "Majestic and powerful, it takes pride,\n",
      "Its"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n\\nIn the tranquil skies so bright,\\nA symphony of feathers take flight.\\nTheir melodious songs fill the air,\\nA gentle whisper from afar.\\n\\nFrom the smallest hummingbird's flit,\\nTo the majestic eagle's might,\\nEach bird in its own way, so divine,\\nAn ode to life and the great design.\\n\\nThe robin, a harbinger of spring,\\nIts red breast adorning its wings,\\nSings with joy as it takes its place,\\nIn nature's grand and wondrous space.\\n\\nThe sparrow, humble yet steadfast,\\nLives in harmony, never boastful or hasty,\\nA lesson for us all to learn,\\nTo cherish what is precious, but still unseen.\\n\\nThe swift and graceful swallows fly,\\nWith a speed that leaves the eye to wonder why,\\nLike shadows darting across the sunlit field,\\nTheir agility and beauty never fail to yield.\\n\\nThe soaring eagle on its wings so wide,\\nMajestic and powerful, it takes pride,\\nIts\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# llm.invoke(\"Make a poem on birds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "QUERY_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"You are an AI language model assistant. Your task is to generate five\n",
    "    different versions of the given user question to retrieve relevant documents from\n",
    "    a vector database. By generating multiple perspectives on the user question, your\n",
    "    goal is to help the user overcome some of the limitations of the distance-based\n",
    "    similarity search. Provide these alternative questions separated by newlines.\n",
    "    Original question: {question}\"\"\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Terms:\n",
    "- Multiqueryretriever: \n",
    "    - automates process of tuning\n",
    "    - to generate multiple queries from different perspective \n",
    "    - for each query- returns relevant documents,, takes union across all \n",
    "    - Overcomes the limitation of distance based retrieval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Retrievers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG prompt\n",
    "template = \"\"\"Answer the question based ONLY on the following context:\n",
    "{context}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig()\n",
    "logging.getLogger(\"langchain.retrievers.multi_query\").setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever()\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Earned leave that must be taken every 2 years, credited annually."
     ]
    }
   ],
   "source": [
    "chain = (\n",
    "    {\"context\": retriever| format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "result=chain.invoke(\"Tell me more about PL in 10 words only\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nPrivilege Leave (PL) is a type of earned leave provided by an organization to its employees. The entitlement and earning of PL are based on the number of months of service rendered by the employee. For example, an employee earns PL at the rate of 1.75 days for every month of service.\\n\\nThe credit of PL is given to the employee's Leave account at the end of each Leave Year, which starts from the 1st of January of the next Leave Year. Employees are required to compulsorily avail at least 15 days of PL in a block of 2 years. This period is referred to as the first block of 2 years and starts from the January when 21 days of leave are earned for those who have recently joined the organization. Any unavailed compulsory PL will lapse in the Leave Year following the year in which compulsory PL was due.\\n\\nEmployees can avail advance PL through an appropriate channel, but the settled amount will be recovered from their PL balance that will be credited in the next Leave Year. The maximum number of days of leave at credit for an employee cannot exceed 90 days. Any amount in\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'questions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m answer \u001b[38;5;241m=\u001b[39mre\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[0-9]\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m,questions)\n\u001b[0;32m      3\u001b[0m question_list \u001b[38;5;241m=\u001b[39m answer\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m question_list\n",
      "\u001b[1;31mNameError\u001b[0m: name 'questions' is not defined"
     ]
    }
   ],
   "source": [
    "import re\n",
    "answer =re.sub(r'[0-9]','',questions)\n",
    "question_list = answer.split('\\n')\n",
    "question_list\n",
    "chain.invoke(question_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Taking too much time'''\n",
    "# answer_list = []\n",
    "# for question in question_list:\n",
    "#     ans = chain.invoke(question)\n",
    "#     answer_list.append(ans)\n",
    "print(\"ANSWERS-----\")\n",
    "# print(answer_list)\n",
    "--------------------------------------------(2)--------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
