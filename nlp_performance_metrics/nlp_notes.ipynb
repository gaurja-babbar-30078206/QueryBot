{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "LINKs:\n",
    "-> https://freedium.cfd/https://towardsdatascience.com/the-most-common-evaluation-metrics-in-nlp-ced6a763ac8b\n",
    "\n",
    "\n",
    "Aim: Whatever you learn, you must learn it\n",
    "in such a way that could teach it to someone else.\n",
    "That's how you will be able to crack your interviews\n",
    "\n",
    "The most common evaluation metrics in NLP:\n",
    "- The type of evaluation metric depends upon the type of NLP task being performed.\n",
    "- Intrinsic metrics:\n",
    "Accuracy\n",
    "Precision\n",
    "Recall\n",
    "F1 Score\n",
    "Area under the curve(AUC)\n",
    "Mean Reciprocal Rank (MRR)\n",
    "Mean Average Precision (MAP)\n",
    "Root Mean Squared Error (RMSE)\n",
    "Mean Absolute Percentage Error (MAPE)\n",
    "METEOR(Metric for Evaluation of Translation with Explicit ORdering)\n",
    "ROUGE(Retrieval-Oriented Understudy for Gisting Evaluation)\n",
    "Perplexity\n",
    "\n",
    "Different evaluation metrics are used depending upon the NLP task\n",
    "\"\"\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# BLEU\n",
    "(Bilingual Evaluation Understudy)\n",
    "\n",
    "Performance metric used in case of Informational Retrieval Tasks\n",
    "BLEU (Bilingual Evaluation Understudy):\n",
    "Link:\n",
    " - MEDIUM ARTICLE: https://machinelearningmastery.com/calculate-bleu-score-for-text-python/#:~:text=The%20Bilingual%20Evaluation%20Understudy%20Score,in%20a%20score%20of%200.0.\n",
    " - PAPER LINK:\n",
    "- typically used to evaluates Machine-translation tasks\n",
    "- but also used for tasks such as text generation, paraphrase generation\n",
    "and text summarization\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Dive BLEU\n",
    "- BLEU score used for evaluating and scoring candidate text using NLTK lib in Python\n",
    "- It is a metric for evaluating a generated sentence to a reference sentence\n",
    "- max score of 1.0 of perfect match, min score of 0.0\n",
    "-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Dive ROUGE:\n",
    "\n",
    "LINKS:\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is the maths behind finding out rouge1, rouge2, rougeL and rougeLsum?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    -rouge1\n",
    "    -rouge2\n",
    "    -rougeL\n",
    "    -rougeLsum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Unlike Bleu, Rouge uses both recall and precision to compare the candidate and reference summaries.\n",
    "- ROUGE is case insensitive,\n",
    "- ROUGE values are in the range of 0 to 1 \n",
    "- the metric is a wrapper around Google Research reimplementation of ROUGE,\n",
    "- Inputs to the package\n",
    "- rouge_types --> defaults to all of them\n",
    "- What is use_aggregator ? \n",
    "- What is use_stemmer ?? boolean, uses Porter Stemmer to strip word suffixes. Defaults to False\n",
    "- The Google reimplementation does not include the stopward removal\n",
    "- two flavors of Rouge are described\n",
    "- sentence level - to compute the Longest common subsequence between two pieces of text. Newlines are ignored. This is called rougeL. so we ignoring the new lines and computing the longest common subsequence.\n",
    "- Summary level : \n",
    "---> Newlines in the text are interpreted as sentence boundaries. and the LCS is computerd between each pair of reference and candidate sentences and something called union-LCS is computed. This called rougeLsum -- rougeLsummary, this is the rougeL reported in Get to the point, summarization with Pointer Generator networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I do not understand rougeLsum ???\n",
    "- it uses newlines as sentence boundaries\n",
    "- how is it computed ??\n",
    "- ROUGE L SUM\n",
    "- finding the LCS between each pair of reference and candidate\n",
    "- Then calculateing the UNION LCS \n",
    "- In this way rougeLsum is found\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Parameter__:\n",
    "1) use_aggregator: True\n",
    "- returns a single averaged out result for each of the rouge types\n",
    "- If set to False, returns a list of rouge result for each sentence in the predictions list.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROUGE SUMMARY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Two packages can be used\n",
    "1) evaluate 2)rouge_scorer\n",
    "Q) What is the difference between the two?\n",
    "A) Rouge function in evaluate only returns fmeasure, whereas in rouge_scorer it returns precision,recall and fmeasure in a Score object which is accessible\n",
    "   But in the rouge function of evaluate we can pass entire list of ref and candidate, but only single string is acceptable by rouge_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import evaluate\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# rouge = evaluate.load('rouge', use_aggregator = False)\n",
    "\n",
    "predictions = [\"Transformers Transformers are fast plus efficient\", \n",
    "            #    \"Good Morning\", \n",
    "            #    \"I am waiting for new Transformers\"\n",
    "               ]\n",
    "references = [\n",
    "              [\"HuggingFace Transformers are fast efficient plus awesome\", \n",
    "               \"Transformers are awesome because they are fast to execute\"], \n",
    "            #   [\"Good Morning Transformers\", \"Morning Transformers\"], \n",
    "            #   [\"People are eagerly waiting for new Transformer models\", \n",
    "            #    \"People are very excited about new Transformers\"]\n",
    "\n",
    "]\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(rouge_types= [ 'rouge1','rouge2', 'rougeL', 'rougeLsum'])\n",
    "results = scorer.score(target= references[0][0], prediction= predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score(precision=0.8333333333333334, recall=0.7142857142857143, fmeasure=0.7692307692307692)\n",
      "Score(precision=0.4, recall=0.3333333333333333, fmeasure=0.3636363636363636)\n",
      "Score(precision=0.6666666666666666, recall=0.5714285714285714, fmeasure=0.6153846153846153)\n",
      "Score(precision=0.6666666666666666, recall=0.5714285714285714, fmeasure=0.6153846153846153)\n"
     ]
    }
   ],
   "source": [
    "for key in results.keys():\n",
    "    print(results[key])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openpyxl\n",
    "import pandas as pd\n",
    "\n",
    "xls = pd.ExcelFile(path)\n",
    "sheets = xls.sheet_names\n",
    "path = r\"D:\\OneDrive - Adani\\Desktop\\LEARNING_FOLDER\\_Kolkata_2024\\1_LLM\\3_Text_query_bot\\_docs\\benchmark_qa_3_temp_zero\\llm_responses_document_query_bot.xlsx\"\n",
    "test_wb = openpyxl.load_workbook(path)\n",
    "\n",
    "test_wb[sheets[0]].insert_cols()\n",
    "test_wb.save(path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
