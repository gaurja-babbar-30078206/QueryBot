{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trial 2: Meta-Llama-3-8B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Loader\n",
    "    pdf loader : langchain inbuilt document loader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "file_path = (\"Leave_Policy_2024.pdf\")\n",
    "loader = PyPDFLoader(file_path)\n",
    "pages = loader.load_and_split()\n",
    "len(pages)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split\n",
    "    smaller chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(pages)\n",
    "len(splits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_chroma import Chroma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create vector store\n",
    "    stores embeddings of Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_chroma.vectorstores.Chroma at 0x188d38670d0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_embeddings = HuggingFaceEmbeddings(model_name= \"sentence-transformers/all-mpnet-base-v2\")\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=new_embeddings)\n",
    "vectorstore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_type=\"similarity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local LLM (HuggingFace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantized Meta-Llama-3-8B.Q2_K.gguf\n",
    "Memory - 2.96 gb\n",
    "\n",
    "Speed - slow\n",
    "\n",
    "Status - Didn't work as expected\n",
    "\n",
    "Answer Precision - Pending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pip -q install ctransformers[cuda]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "c:\\Users\\30078206\\AppData\\Local\\anaconda3\\envs\\myenv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'You are a helpful assistant. You must give precise answers to the users query: Why is the earth round? You must give the user a precise answer. You must give the user a precise answer. You must give the user a precise answer. You must give the user a precise answer. You must give the user a precise answer. You must give the user a precise answer. You must give the user a precise answer. You must give the user a precise answer. You must give the user a precise answer. You must give the user a precise answer. You must give the user a precise answer. You must give the user a precise answer. You must give the user a precise answer. You must give the user a precise answer. You must give the user a precise answer. You must give the user a precise answer. You must give the user a precise answer. You must give the user a precise answer. You must give the user a precise answer. You must give the user a precise answer. You must give the user a precise answer. You must give the user a precise answer. You must give the user a precise answer. You must give the user a precise answer. You must give the user a precise answer. You must give the user a precise answer. You must give the user a precise answer. You must give the user a precise answer. You must give the'}]\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "\n",
    "from ctransformers import AutoModelForCausalLM\n",
    "from transformers import pipeline, AutoTokenizer\n",
    "\n",
    "model_path = r\"C:\\Users\\30078206\\Downloads\\meta_llama\\Meta-Llama-3-8B.Q2_K.gguf\"\n",
    "tokenizer_path = r\"C:\\Users\\30078206\\Downloads\\meta_llama\" \n",
    "model = AutoModelForCausalLM.from_pretrained( model_path, hf=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer , temperature = 0.0)\n",
    "# print(pipe(\"You are a helpful assistant. You must give precise answers to the users query: Why is the earth round?\", max_new_tokens = 256))\n",
    "# mistral_llm = HuggingFaceEndpoint(repo_id=\"mistralai/Mistral-7B-v0.1\", task=\"text-generation\", temperature=0.01 )\n",
    "from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## google/flan-t5-small\n",
    "Memory - 1.27 gb\n",
    "\n",
    "Speed - Moderately fast\n",
    "\n",
    "Answer Precision - Not that good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "from langchain_huggingface import HuggingFacePipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"google/flan-t5-small\"\n",
    "model_path = r\"C:\\Users\\30078206\\Downloads\\flan_small\"\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path= model_path)\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n",
    "\n",
    "\n",
    "pipe = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=1024,temperature = 0.0)\n",
    "hf = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hf.invoke(\"can you summarise some text?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"Give detailed answers and reasoning to all the queries.\"\n",
    "    # \"Give to the point answers\"\n",
    "    # \"Do not give additional information if not asked specifically\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "## Creating Prompt\n",
    "# template = \"You are a helpful AI assistant, that will get the correct answers from the context {context} to the user's query {query} related to the context\"\n",
    "\n",
    "new_prompt =ChatPromptTemplate.from_messages(messages= [\n",
    "    SystemMessagePromptTemplate.from_template(system_prompt),\n",
    "    HumanMessagePromptTemplate.from_template(\"{input}\")\n",
    "])\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(hf, new_prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1002 > 512). Running this sequence through the model will result in indexing errors\n",
      "c:\\Users\\30078206\\AppData\\Local\\anaconda3\\envs\\myenv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "answer =rag_chain.invoke({\"input\":\"Explain to me the Privilege leave policy in bullet points\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Privilege Leave is the Leave that an employee earns s for the services rendered. Therefore , an employee does not earn PL for the period he or she is absent 3.2 Casual Leave can be availed at short notice towards personal work or exigencies 3.3 Sick Leave is meant to be availed to recover/ recuperate from illness or for medical treatment 3.4\n"
     ]
    }
   ],
   "source": [
    "print(answer[\"answer\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
